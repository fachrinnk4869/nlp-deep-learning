{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the 'google/fleurs' dataset for 'hi_in' (Hindi - India)\n",
    "dset = load_dataset(\"SEACrowd/indo_general_mt_en_id\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_data = dset['train']\n",
    "val_data = dset['validation']\n",
    "test_data = dset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSoGDNUH3310"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, SimpleRNN, Embedding, Dense, Attention\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "# Randomly select 10k indices\n",
    "random_indices = random.sample(range(len(train_data)), 10000)\n",
    "\n",
    "# Select the 10k rows\n",
    "train_data = train_data.select(random_indices)\n",
    "# Extract source (Indonesian) and target (English) texts from each split\n",
    "train_source_texts = [example['src'] for example in train_data]\n",
    "train_target_texts = [example['tgt'] for example in train_data]\n",
    "\n",
    "val_source_texts = [example['src'] for example in val_data]\n",
    "val_target_texts = [example['tgt'] for example in val_data]\n",
    "\n",
    "test_source_texts = [example['src'] for example in test_data]\n",
    "test_target_texts = [example['tgt'] for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens to your training data\n",
    "train_target_texts = [\"<SOS> \" + text + \" <EOS>\" for text in train_target_texts]\n",
    "val_target_texts = [\"<SOS> \" + text + \" <EOS>\" for text in val_target_texts]\n",
    "test_target_texts = [\"<SOS> \" + text + \" <EOS>\" for text in test_target_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erDDGIBR3314"
   },
   "outputs": [],
   "source": [
    "# Tokenization (Source and Target)\n",
    "source_tokenizer = Tokenizer()\n",
    "target_tokenizer = Tokenizer()\n",
    "\n",
    "source_tokenizer.fit_on_texts(train_source_texts)\n",
    "target_tokenizer.fit_on_texts(train_target_texts)\n",
    "\n",
    "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "# Convert texts to sequences\n",
    "train_source_sequences = source_tokenizer.texts_to_sequences(train_source_texts)\n",
    "train_target_sequences = target_tokenizer.texts_to_sequences(train_target_texts)\n",
    "\n",
    "val_source_sequences = source_tokenizer.texts_to_sequences(val_source_texts)\n",
    "val_target_sequences = target_tokenizer.texts_to_sequences(val_target_texts)\n",
    "\n",
    "test_source_sequences = source_tokenizer.texts_to_sequences(test_source_texts)\n",
    "test_target_sequences = target_tokenizer.texts_to_sequences(test_target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define maximum sequence lengths (modify as needed)\n",
    "# max_source_length = max(len(seq) for seq in train_source_sequences)\n",
    "# max_target_length = max(len(seq) for seq in train_target_sequences)\n",
    "max_source_length = 60\n",
    "max_target_length = 60\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "train_source_sequences = pad_sequences(train_source_sequences, maxlen=max_source_length, padding='post')\n",
    "train_target_sequences = pad_sequences(train_target_sequences, maxlen=max_target_length, padding='post')\n",
    "\n",
    "val_source_sequences = pad_sequences(val_source_sequences, maxlen=max_source_length, padding='post')\n",
    "val_target_sequences = pad_sequences(val_target_sequences, maxlen=max_target_length, padding='post')\n",
    "\n",
    "test_source_sequences = pad_sequences(test_source_sequences, maxlen=max_source_length, padding='post')\n",
    "test_target_sequences = pad_sequences(test_target_sequences, maxlen=max_target_length, padding='post')\n",
    "\n",
    "# Create TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_source_sequences, train_target_sequences))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_source_sequences, val_target_sequences))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_source_sequences, test_target_sequences))\n",
    "\n",
    "# Batch and shuffle the training dataset\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(batch_size, drop_remainder=True)\n",
    "test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# Print dataset samples to verify\n",
    "for source, target in train_dataset.take(1):\n",
    "    print('Source batch shape:', source.shape)\n",
    "    print('Target batch shape:', target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLWrfTYv3319",
    "outputId": "e6065eb5-a168-4017-f85f-3038deb49cf6"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Encoder\n",
    "#\n",
    "# Encoder with LSTM\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            self.enc_units,\n",
    "            return_sequences=False,  # Set to True if you need the full sequence\n",
    "            return_state=True,       # Set to True to get the last state (hidden and cell)\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "            unroll=True  # Forces TensorFlow to use non-cuDNN kernels\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Inside call method of Decoder\n",
    "        print(\"Before embedding:\", x.shape)\n",
    "        x = self.embedding(x)\n",
    "        print(\"After embedding:\", x.shape)\n",
    "        output, state_hidden, state_cell = self.lstm(x)\n",
    "        print(\"After LSTM: output shape:\", output.shape, \n",
    "            \"state_hidden shape:\", state_hidden.shape,\n",
    "            \"state_cell shape:\", state_cell.shape)\n",
    "        print(\"After FC layer:\", x.shape)\n",
    "\n",
    "        return state_hidden, state_cell  # Return both hidden and cell states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fy-2e-TR332A"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Decoder\n",
    "#\n",
    "# Decoder with LSTM\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            self.dec_units,\n",
    "            return_sequences=True,  # Set to True to return the full sequence\n",
    "            return_state=True,      # Set to True to return the last hidden and cell states\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "            unroll=True  # Forces TensorFlow to use non-cuDNN kernels\n",
    "        )\n",
    "        self.softmax = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, x, hidden, cell):\n",
    "        x = self.embedding(x)\n",
    "        # Pass both hidden and cell states to the LSTM\n",
    "        output, hidden_state, cell_state = self.lstm(inputs=x, initial_state=[hidden, cell])\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "\n",
    "        return output, hidden_state, cell_state  # Return output, hidden state, and cell state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Create model\n",
    "# ========================================\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024  # LSTM/GRU dimensionality of the output space.\n",
    "\n",
    "encoder = Encoder(source_vocab_size, embedding_dim, units, batch_size)\n",
    "decoder = Decoder(target_vocab_size, embedding_dim, units, batch_size)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"val_accuracy\")\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0)) # this masks '<pad>'\n",
    "    \"\"\"\n",
    "    Example:\n",
    "\n",
    "    real= tf.Tensor(\n",
    "    [[21  1 44 0  0]   (jump !    <eos> <pad> <pad>)\n",
    "    [ 17  9 24 2 44]   (i    go   there .     <eos>)\n",
    "    [ 27  1 44 0  0]   (no   !    <eos> <pad> <pad>)\n",
    "    [ 21 22 32 2 44]], (i    know you   .     <eos>)\n",
    "    , shape=(4, 5), dtype=int64)\n",
    "\n",
    "    where <pad> = 0.\n",
    "\n",
    "    mask= tf.Tensor(\n",
    "    [[True  True  True False False]\n",
    "    [ True  True  True True  True ]\n",
    "    [[True  True  True False False]\n",
    "    [ True  True  True True  True ],\n",
    "    shape=(4, 5), dtype=bool)\n",
    "    \"\"\"\n",
    "\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Training\n",
    "# ========================================\n",
    "import os \n",
    "import csv\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "checkpoint_path = (\n",
    "    \"./checkpointsLLSTM/seq2seq-sample-\"\n",
    "    + str(500)\n",
    "    + \"-embedding-\"\n",
    "    + str(embedding_dim)\n",
    "    + \"-hidden-\"\n",
    "    + str(units)\n",
    ")\n",
    "\n",
    "if CHECKPOINT == True:\n",
    "    ckpt = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "        print(\"Latest checkpoint restored!!\")\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Compute BLEU scores for a batch of references and hypotheses.\n",
    "    `references`: List of lists of reference sentences (tokenized).\n",
    "    `hypotheses`: List of hypothesis sentences (tokenized).\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        # BLEU score for each sentence\n",
    "        scores.append(sentence_bleu([ref], hyp))\n",
    "    return sum(scores) / len(scores)  # Average BLEU score for the batch\n",
    "\n",
    "def append_loss_to_csv(epoch, train_loss, val_loss, time_taken, csv_path=\"training_lstm_results.csv\"):\n",
    "    file_exists = os.path.exists(csv_path)\n",
    "    with open(csv_path, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            # Write header if the file is being created for the first time\n",
    "            writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Time Taken\"])\n",
    "        # Append the new epoch's losses\n",
    "        writer.writerow([epoch, train_loss.numpy(), val_loss.numpy(), time_taken])\n",
    "    print(f\"Epoch {epoch + 1}:Val Loss: {val_loss:.4f} - Saved to {csv_path}.\")\n",
    "\n",
    "@tf.function\n",
    "def train(encoder, decoder, source_sentences, target_sentences, target_lang_tokenizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encoder outputs\n",
    "        h, c = encoder(source_sentences)\n",
    "\n",
    "        # Decoder inputs\n",
    "        dec_input = target_sentences[:, :-1]  # Input for decoder\n",
    "        expected_dec_output = target_sentences[:, 1:]  # Target for comparison\n",
    "\n",
    "        # Debug inputs\n",
    "        print(\"dec_input shape:\", dec_input.shape, \"dec_input dtype:\", dec_input.dtype)\n",
    "        print(\"h shape:\", h.shape, \"h dtype:\", h.dtype)\n",
    "        print(\"c shape:\", c.shape, \"c dtype:\", c.dtype)\n",
    "\n",
    "        # Decode\n",
    "        predictions, _, _ = decoder(dec_input, h, c)\n",
    "\n",
    "        # Debug outputs\n",
    "        print(\"Predictions shape:\", predictions.shape)\n",
    "        loss = loss_function(expected_dec_output, predictions)\n",
    "        train_accuracy(expected_dec_output, predictions)\n",
    "\n",
    "    batch_loss = loss / int(target_sentences.shape[1])\n",
    "    variables = encoder.variables + decoder.variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def validate(encoder, decoder, source_sentences, target_sentences, target_lang_tokenizer):\n",
    "    h, c = encoder(source_sentences)\n",
    "    dec_input = target_sentences[:, :-1]  # Decoder input\n",
    "    expected_dec_output = target_sentences[:, 1:]  # Expected output\n",
    "\n",
    "    predictions, _, _ = decoder(dec_input, h, c)\n",
    "    loss = loss_function(expected_dec_output, predictions)\n",
    "    val_accuracy(expected_dec_output, predictions)  # Assuming you have val_accuracy defined\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Set n_epochs at least 20 when you do training.\n",
    "n_epochs = 40\n",
    "\n",
    "# Prepare to store loss and accuracy\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "val_loss_history = []\n",
    "val_accuracy_history = []\n",
    "\n",
    "import time\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    start = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_val_loss = 0\n",
    "    train_accuracy.reset_state()\n",
    "    val_accuracy.reset_state()  # Reset validation accuracy\n",
    "\n",
    "    # Training loop\n",
    "    for (batch, (source_sentences, target_sentences)) in enumerate(train_dataset):\n",
    "        batch_loss = train(encoder, decoder, source_sentences, target_sentences, target_tokenizer)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy: {:.4f}\".format(epoch, batch, batch_loss.numpy(), train_accuracy.result()))\n",
    "\n",
    "    # Store the average loss and accuracy for this epoch\n",
    "    avg_loss = total_loss / (batch + 1)\n",
    "    loss_history.append(avg_loss)\n",
    "    accuracy_history.append(train_accuracy.result().numpy())\n",
    "\n",
    "    # Validation loop\n",
    "    for (val_source_sentences, val_target_sentences) in val_dataset:\n",
    "        val_loss = validate(encoder, decoder, val_source_sentences, val_target_sentences, target_tokenizer)\n",
    "        total_val_loss += val_loss\n",
    "\n",
    "    # Store the average validation loss and accuracy for this epoch\n",
    "    avg_val_loss = total_val_loss / len(val_dataset)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    # calculate bleu score\n",
    "    if CHECKPOINT == True:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print(\"Saving checkpoint for epoch {} at {}\".format(epoch, ckpt_save_path))\n",
    "    time_taken = time.time() - start\n",
    "    append_loss_to_csv(epoch, avg_loss, avg_val_loss, time_taken)\n",
    "    print(\"Epoch {}/{} Loss {:.4f} Val Loss {:.4f}\".format(epoch, n_epochs, avg_loss, avg_val_loss))\n",
    "    print(\"Time taken for 1 epoch {:.4f} sec\\n\".format(time_taken))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting Loss and Accuracy\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss_history, label='Training Loss', color='blue')\n",
    "plt.plot(val_loss_history, label='Validation Loss', color='orange')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(accuracy_history, label='Training Accuracy', color='green')\n",
    "plt.plot(val_accuracy_history, label='Validation Accuracy', color='red')\n",
    "plt.title('Training and Validation Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction, sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Translation\n",
    "# ========================================\n",
    "import csv\n",
    "def evaluate(sentence, encoder, decoder, source_lang_tokenizer, target_lang_tokenizer):\n",
    "\n",
    "    inputs = source_lang_tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "    inputs = tf.compat.v1.keras.preprocessing.sequence.pad_sequences(\n",
    "        [inputs], maxlen=max_source_length, padding=\"post\"\n",
    "    )\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = \"\"\n",
    "\n",
    "    h, c = encoder(inputs)\n",
    "    dec_input = tf.expand_dims([target_lang_tokenizer.word_index[\"sos\"]], 0)\n",
    "\n",
    "    for t in range(max_target_length):\n",
    "        #\n",
    "        # Greedy Search\n",
    "        #\n",
    "        predictions, h, c = decoder(dec_input, h, c)\n",
    "        predicted_id = tf.argmax(predictions[0][0]).numpy()\n",
    "        result += target_lang_tokenizer.index_word[predicted_id] + \" \"\n",
    "        if target_lang_tokenizer.index_word[predicted_id] == \"eos\":\n",
    "            return result\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder, source_lang_tokenizer, target_lang_tokenizer):\n",
    "    result = evaluate(sentence, encoder, decoder, source_lang_tokenizer, target_lang_tokenizer)\n",
    "    return result.capitalize()\n",
    "\n",
    "\"\"\"\n",
    "# for debug:\n",
    "#sentence = \"Su voz suena muy bello.\"\n",
    "#sentence = \"No nos gusta la lluvia.\"\n",
    "sentence = \"Nos gusta la lluvia.\"\n",
    "result = translate(sentence, encoder, decoder, source_lang_tokenizer, target_lang_tokenizer)\n",
    "print(\"Input    : {}\".format(sentence))\n",
    "print(\"Predicted: {}\".format(result))\n",
    "\n",
    "sys.exit()\n",
    "\"\"\"\n",
    "#\n",
    "#\n",
    "# Helper function to detokenize a sequence to text\n",
    "def detokenize(sequence, tokenizer):\n",
    "    return ' '.join([tokenizer.index_word.get(idx, '') for idx in sequence if idx != 0])\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    # Tokenize the sentences for BLEU score calculation\n",
    "\n",
    "    # Use the sentence_bleu function to calculate the score\n",
    "    bleu_score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method1)\n",
    "    return bleu_score\n",
    "keys = np.arange(len(test_source_sequences))\n",
    "bleu_scores = []\n",
    "for i in range(len(keys)):\n",
    "    print(\"===== [{}] ======\".format(i + 1))\n",
    "    sentence = detokenize(test_source_sequences[i], source_tokenizer)\n",
    "    result = translate(sentence, encoder, decoder, source_tokenizer, target_tokenizer)\n",
    "    print(\"Input    : {}\".format(sentence))\n",
    "    print(\"Predicted: {}\".format(result))\n",
    "    correct_sentence = detokenize(test_target_sequences[i], target_tokenizer)\n",
    "    print(\"Correct  : {}\".format(correct_sentence))\n",
    "    # Calculate and store the BLEU score\n",
    "    bleu_score = calculate_bleu(correct_sentence, result)\n",
    "    bleu_scores.append(bleu_score)\n",
    "    print(\"BLEU Score: {:.4f}\".format(bleu_score))\n",
    "    # Save the BLEU scores to a CSV file\n",
    "    with open('predictions_lstm_test.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([i + 1, sentence, result, correct_sentence, bleu_score])\n",
    "# Calculate the average BLEU score for the sample\n",
    "average_bleu_score = np.mean(bleu_scores)\n",
    "print(\"Average BLEU Score for the sample: {:.4f}\".format(average_bleu_score))\n",
    "# encoder.summary()\n",
    "# decoder.summary()\n",
    "# decoder.get_config()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
